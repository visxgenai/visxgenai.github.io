(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{7345:(e,s,i)=>{"use strict";i.d(s,{default:()=>t});var a=i(5155);function t(){return(0,a.jsxs)("section",{id:"schedule",className:"section",children:[(0,a.jsx)("h2",{className:"text-3xl font-bold text-gray-800 mb-4",children:"Schedule"}),(0,a.jsx)("p",{className:"text-gray-700 mb-4",children:"Keynote speakers will be announced soon! "}),(0,a.jsx)("div",{className:"overflow-x-auto",children:(0,a.jsx)("table",{className:"min-w-full border border-gray-300",children:(0,a.jsxs)("tbody",{className:"divide-y divide-gray-200",children:[(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top font-bold",children:["Vienna Time (UTC+2)",(0,a.jsx)("br",{}),(0,a.jsx)("span",{className:"text-sm text-gray-600",children:"(10 min)"})]}),(0,a.jsx)("td",{className:"px-4 py-2 font-bold",children:"Events"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top",children:["8:30 – 8:40 am",(0,a.jsx)("br",{})]}),(0,a.jsx)("td",{className:"px-4 py-2",children:"Introduction of workshop organizers, participants, topics, and goals"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top",children:["8:40 – 9:30 am",(0,a.jsx)("br",{})]}),(0,a.jsx)("td",{className:"px-4 py-2",children:"Keynote by the invited speaker (discussion and Q&A included)"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top",children:["9:30 – 10:00 am",(0,a.jsx)("br",{})]}),(0,a.jsxs)("td",{className:"px-4 py-2",children:["Paper presentation: challenge winners, case studies",(0,a.jsx)("br",{}),(0,a.jsx)("span",{className:"text-sm text-gray-600",children:"(3\xd710 min, Q&A included)"})]})]}),(0,a.jsx)("tr",{children:(0,a.jsx)("td",{colSpan:2,className:"py-2 text-center font-medium text-gray-700",children:"30 min break"})}),(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top",children:["10:30 – 11:20 am",(0,a.jsx)("br",{})]}),(0,a.jsx)("td",{className:"px-4 py-2",children:"Keynote by the invited speaker (discussion and Q&A included)"})]}),(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top",children:["11:20 – 11:50 am",(0,a.jsx)("br",{})]}),(0,a.jsxs)("td",{className:"px-4 py-2",children:["Paper presentation: design lessons, tools",(0,a.jsx)("br",{}),(0,a.jsx)("span",{className:"text-sm text-gray-600",children:"(3\xd710 min, Q&A included)"})]})]}),(0,a.jsxs)("tr",{children:[(0,a.jsxs)("td",{className:"px-4 py-2 align-top",children:["11:50 am – 12:00 pm",(0,a.jsx)("br",{})]}),(0,a.jsx)("td",{className:"px-4 py-2",children:"Workshop summary"})]})]})})})]})}i(2115)},8423:(e,s,i)=>{"use strict";i.d(s,{default:()=>n});var a=i(5155),t=i(2115);function n(){let[e,s]=(0,t.useState)({}),i=e=>{s(s=>({...s,[e]:!s[e]}))},n={genaiInterpretability1:["Smith et al. (2024). VisLLM: A Framework for Visualizing Language Model Decision Processes","Jones & Wong (2023). Diffusion Lens: Interactive Visualizations for Understanding Image Generation","Martinez et al. (2024). Attention Flow: Visual Analysis of Transformer Attention Patterns"],genaiInterpretability2:["Chen et al. (2024). Beyond Attention: New Interpretability Methods for Modern Language Models","Garcia & Kim (2023). Graph-based Visualization of LLM Knowledge Extraction","Williams et al. (2024). Visual Analytics for Multimodal Model Understanding"],genaiInterpretability3:["Lee & Patel (2023). The Visualization Gap: Challenges for Next-Gen AI Interpretability","Thompson et al. (2024). VISAI: A Research Agenda for Visual Analytics in Generative AI","Rodriguez et al. (2024). Benchmarking Visualization Literacy for AI Interpretability Tools"]};return(0,a.jsxs)("div",{className:"mb-8 section",id:"cfp",children:[(0,a.jsx)("h1",{className:"text-3xl font-bold text-gray-800 mb-4",children:"Call for Participants"}),(0,a.jsx)("p",{className:"mb-5",children:"We invite participation through two submission tracks: short paper and mini-challenge. Both are opportunities to showcase novel ideas and engage with the growing community at the intersection of visualization, generative AI, and agentic systems."}),(0,a.jsx)("section",{className:"mb-5",children:(0,a.jsxs)("div",{className:"mb-5",children:[(0,a.jsx)("h2",{className:"text-2xl mb-4 font-bold",children:"Paper Submission"}),(0,a.jsx)("p",{className:"mb-4",children:"We invite short papers (2–4 pages, non-archival). Submissions will be reviewed by at least two reviewers. Accepted papers will be invited to present as posters, demos, or lightning talks during the workshop, and published on the workshop website."}),(0,a.jsx)("p",{className:"mb-4",children:"We encourage diverse contributions across theory, systems, user studies, and applications, connecting VIS with GenAI or agentic workflows. Topics include (but are not limited to):"}),(0,a.jsxs)("div",{className:"mb-5",children:[(0,a.jsx)("h4",{className:"mb-2 font-bold",children:"VIS X GenAI Interpretability"}),(0,a.jsxs)("ul",{className:"list-disc pl-5 mb-4",children:[(0,a.jsxs)("li",{className:"mb-3",children:["Novel visualization systems and techniques for interpreting frontier generative models, such as LLMs or diffusion models",(0,a.jsxs)("div",{className:"mt-1",children:[(0,a.jsxs)("button",{onClick:()=>i("genaiInterpretability1"),className:"text-sm text-blue-600 hover:text-blue-800 flex items-center",children:[(0,a.jsx)("span",{children:e.genaiInterpretability1?"▼":"▶"}),(0,a.jsx)("span",{className:"ml-1",children:"Example papers"})]}),e.genaiInterpretability1&&(0,a.jsx)("ul",{className:"list-disc pl-8 mt-2 text-sm text-gray-700 p-2 rounded",children:n.genaiInterpretability1.map((e,s)=>(0,a.jsx)("li",{className:"mb-1",children:e},s))})]})]}),(0,a.jsxs)("li",{className:"mb-3",children:["Interpretability-focused papers from the GenAI community that highlight technical challenges or opportunities where visualization can help",(0,a.jsxs)("div",{className:"mt-1",children:[(0,a.jsxs)("button",{onClick:()=>i("genaiInterpretability2"),className:"text-sm text-blue-600 hover:text-blue-800 flex items-center",children:[(0,a.jsx)("span",{children:e.genaiInterpretability2?"▼":"▶"}),(0,a.jsx)("span",{className:"ml-1",children:"Example papers"})]}),e.genaiInterpretability2&&(0,a.jsx)("ul",{className:"list-disc pl-8 mt-2 text-sm text-gray-700 p-2 rounded",children:n.genaiInterpretability2.map((e,s)=>(0,a.jsx)("li",{className:"mb-1",children:e},s))})]})]}),(0,a.jsxs)("li",{className:"mb-3",children:["Position papers and proposals outlining research agendas, benchmarks, or tools to support future collaboration between VIS and interpretability researchers",(0,a.jsxs)("div",{className:"mt-1",children:[(0,a.jsxs)("button",{onClick:()=>i("genaiInterpretability3"),className:"text-sm text-blue-600 hover:text-blue-800 flex items-center",children:[(0,a.jsx)("span",{children:e.genaiInterpretability3?"▼":"▶"}),(0,a.jsx)("span",{className:"ml-1",children:"Example papers"})]}),e.genaiInterpretability3&&(0,a.jsx)("ul",{className:"list-disc pl-8 mt-2 text-sm text-gray-700 p-2 rounded",children:n.genaiInterpretability3.map((e,s)=>(0,a.jsx)("li",{className:"mb-1",children:e},s))})]})]})]})]}),(0,a.jsxs)("div",{className:"mb-3",children:[(0,a.jsx)("h4",{className:"mb-2 font-bold",children:"Agentic Systems and VIS"}),(0,a.jsxs)("ul",{className:"list-disc pl-5 mb-3",children:[(0,a.jsx)("li",{className:"mb-2",children:"Agent-augmented VIS tools: agents that recommend, generate, or evaluate visualizations for human users"}),(0,a.jsx)("li",{className:"mb-2",children:"VIS tools for agents: encodings and UIs that agents themselves can perceive, reason over, or act upon"}),(0,a.jsx)("li",{className:"mb-2",children:"Benchmarks and evaluations for assessing agent performance on VIS-related tasks"}),(0,a.jsx)("li",{className:"mb-2",children:"Case studies and demos of agent systems applied to real-world visual analysis workflows"}),(0,a.jsx)("li",{className:"mb-2",children:"Vision papers on agents in VIS education, immersive visualizations for embodied agents, or multi-agent coordination in visual reasoning"})]})]}),(0,a.jsxs)("div",{className:"mb-6",children:[(0,a.jsx)("h4",{className:"text-2xl mb-4 font-bold",children:"Submission & Review"}),(0,a.jsx)("p",{children:"Submissions will be evaluated based on several key criteria to ensure high quality and relevance to the workshop’s focus. Papers will be assessed on: (1) novelty and originality of the contribution; (2) potential impact on frontier model development; (3) technical quality of visualization techniques or systems; (4) interdisciplinary relevance to VIS and GenAI communities; and (5) clarity of presentation. For demos and systems papers, we will additionally review the submitted demos for usability and scalability to real-world frontier model understanding tasks. Position papers will be judged on their potential to stimulate discussion and inform future research directions. All submissions will undergo double-blind review by a program committee comprising experts from both the visualization and AI interpretability communities to ensure balanced evaluation."})]})]})})]})}},9723:(e,s,i)=>{Promise.resolve().then(i.bind(i,8423)),Promise.resolve().then(i.bind(i,7345))}},e=>{var s=s=>e(e.s=s);e.O(0,[441,684,358],()=>s(9723)),_N_E=e.O()}]);