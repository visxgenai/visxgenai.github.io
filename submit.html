<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/f71857d3775b5ef3.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-8f14a0b048916c48.js"/><script src="/_next/static/chunks/4bd1b696-18452535c1c4862d.js" async=""></script><script src="/_next/static/chunks/684-8c7f7c22671a5a40.js" async=""></script><script src="/_next/static/chunks/main-app-06a3cfeec243e990.js" async=""></script><script src="/_next/static/chunks/app/layout-21c87ec4c8d578d1.js" async=""></script><meta name="next-size-adjust" content=""/><title>VISxGenAI</title><meta name="description" content="Workshop on Explainable Visualizations focusing on AI techniques"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d65c78"><div class="flex flex-col min-h-screen"><nav class="bg-[#eff3f7] border-b border-[#c1d1ea]"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between h-16"><div class="flex-shrink-0 flex items-center"><a class="text-[#333] text-xl md:text-2xl" href="/">VISxGenAI</a></div><div class="hidden md:flex items-center space-x-4"><a class="px-3 py-2 rounded-md text-[#333] hover:bg-[#d0dbed] transition-colors text-lg" href="/cfp">Call for Papers</a><a class="px-3 py-2 rounded-md text-[#333] hover:bg-[#d0dbed] transition-colors text-lg" href="/submit">Submit</a><a class="px-3 py-2 rounded-md text-[#333] hover:bg-[#d0dbed] transition-colors text-lg" href="/vision">Vision</a><a class="px-3 py-2 rounded-md text-[#333] hover:bg-[#d0dbed] transition-colors text-lg" href="/agent-playground">Agent playground</a></div><div class="md:hidden flex items-center"><button class="text-[#333] hover:bg-[#d0dbed] p-2 rounded-md focus:outline-none"><span class="sr-only">Open main menu</span><svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></div></nav><main class="flex-grow container mx-auto px-4 py-2 mt-1"><div class="max-w-6xl mx-auto px-4 py-2 font-lato"><h1 class="text-4xl  text-gray-800 mb-6">Submit Your Work</h1><section class="mb-8"><h2 class="text-2xl mb-4">Submission Guidelines</h2><p class="mb-6">We invite participation through two submission tracks: short paper and mini-challenge. Both are opportunities to showcase novel ideas and engage with the growing community at the intersection of visualization, generative AI, and agentic systems. an be up to four pages plus up to one page of references. Submissions will be in the VGTC conference two-column format, in line with the IEEE VIS formatting guidelines. Short Papers should eb double-blind (anonymized) submissions. Double-blind submissions must NOT include author names or institutions on the cover page of the initial submission, and authors should make an effort to ensure that there is no revealing information in the text, such as apparent citations to authors’ previous work, or making acknowledgments to colleagues of long standing.</p><div class="mb-8"><h3 class="text-xl font-semibold mb-3">Mini-Challenge</h3><p class="mb-4">Inspired by challenges like ImageNet in computer vision and HELM in language models, our mini-challenge invites participants to build or adapt AI agents that can automatically analyze datasets and generate visual data reports.</p><p class="mb-4">The goal is to benchmark and accelerate progress in agent-based data analysis and communication. We will provide a starter kit and evaluation setup to support participants. More details—including datasets, templates, and submission instructions—will be released soon.</p><p class="font-semibold italic">Stay tuned!</p></div></section><div class="mt-6"><a href="https://tc.computer.org/vgtc/publications/conference/" target="_blank" rel="noopener noreferrer"><button class="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600 transition-colors">Go to template</button></a></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="bg-[#e0e8f5] border-t border-[#c1d1ea] py-4"><div class="container mx-auto px-4"><div class="flex justify-between items-center"><div class="flex space-x-16"><span class="text-gray-800 text-xl">Logo1</span><span class="text-gray-800 text-xl">Logo2</span></div><div class="flex space-x-16"><span class="text-gray-800 text-xl">Logo3</span><span class="text-gray-800 text-xl">Logo4</span><span class="text-gray-800 text-xl">Logo5</span></div></div></div></footer></div><script src="/_next/static/chunks/webpack-8f14a0b048916c48.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[8088,[\"177\",\"static/chunks/app/layout-21c87ec4c8d578d1.js\"],\"default\"]\n3:I[7555,[],\"\"]\n4:I[1295,[],\"\"]\n5:I[9665,[],\"MetadataBoundary\"]\n7:I[9665,[],\"OutletBoundary\"]\na:I[4911,[],\"AsyncMetadataOutlet\"]\nc:I[9665,[],\"ViewportBoundary\"]\ne:I[6614,[],\"\"]\n:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/f71857d3775b5ef3.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"RZksbKdpWLApebsL4ZgGx\",\"p\":\"\",\"c\":[\"\",\"submit\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"submit\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f71857d3775b5ef3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_d65c78\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col min-h-screen\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-grow container mx-auto px-4 py-2 mt-1\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"bg-[#e0e8f5] border-t border-[#c1d1ea] py-4\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex space-x-16\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-gray-800 text-xl\",\"children\":\"Logo1\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-800 text-xl\",\"children\":\"Logo2\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex space-x-16\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-gray-800 text-xl\",\"children\":\"Logo3\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-800 text-xl\",\"children\":\"Logo4\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-800 text-xl\",\"children\":\"Logo5\"}]]}]]}]}]}]]}]}]}]]}],{\"children\":[\"submit\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 py-2 font-lato\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl  text-gray-800 mb-6\",\"children\":\"Submit Your Work\"}],[\"$\",\"section\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl mb-4\",\"children\":\"Submission Guidelines\"}],[\"$\",\"p\",null,{\"className\":\"mb-6\",\"children\":\"We invite participation through two submission tracks: short paper and mini-challenge. Both are opportunities to showcase novel ideas and engage with the growing community at the intersection of visualization, generative AI, and agentic systems. an be up to four pages plus up to one page of references. Submissions will be in the VGTC conference two-column format, in line with the IEEE VIS formatting guidelines. Short Papers should eb double-blind (anonymized) submissions. Double-blind submissions must NOT include author names or institutions on the cover page of the initial submission, and authors should make an effort to ensure that there is no revealing information in the text, such as apparent citations to authors’ previous work, or making acknowledgments to colleagues of long standing.\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold mb-3\",\"children\":\"Mini-Challenge\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"Inspired by challenges like ImageNet in computer vision and HELM in language models, our mini-challenge invites participants to build or adapt AI agents that can automatically analyze datasets and generate visual data reports.\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The goal is to benchmark and accelerate progress in agent-based data analysis and communication. We will provide a starter kit and evaluation setup to support participants. More details—including datasets, templates, and submission instructions—will be released soon.\"}],[\"$\",\"p\",null,{\"className\":\"font-semibold italic\",\"children\":\"Stay tuned!\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-6\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://tc.computer.org/vgtc/publications/conference/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"$\",\"button\",null,{\"className\":\"bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600 transition-colors\",\"children\":\"Go to template\"}]}]}]]}],[\"$\",\"$L5\",null,{\"children\":\"$L6\"}],null,[\"$\",\"$L7\",null,{\"children\":[\"$L8\",\"$L9\",[\"$\",\"$La\",null,{\"promise\":\"$@b\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"twWkGx65hKQo-u715FCRr\",{\"children\":[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:\"$Sreact.suspense\"\n10:I[4911,[],\"AsyncMetadata\"]\n6:[\"$\",\"$f\",null,{\"fallback\":null,\"children\":[\"$\",\"$L10\",null,{\"promise\":\"$@11\"}]}]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"11:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"VISxGenAI\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Workshop on Explainable Visualizations focusing on AI techniques\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]],\"error\":null,\"digest\":\"$undefined\"}\nb:{\"metadata\":\"$11:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>